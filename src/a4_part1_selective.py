# -*- coding: utf-8 -*-
"""A4_Part1_selective.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sRcO8svNHo64Dj7UxnGuIaYOYpPET6vd
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import time
import copy

model_name = "EleutherAI/gpt-neo-1.3B"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

def compute_quantization_params(min_val, max_val, dtype=torch.int8):

    qmin = torch.iinfo(dtype).min
    qmax = torch.iinfo(dtype).max
    scale = (max_val - min_val) / (qmax - qmin)
    zero_point = qmin - (min_val / scale)
    return scale, zero_point

def quantize(tensor, scale, zero_point):

    quantized = torch.round((tensor / scale) + zero_point).to(torch.int8)
    return quantized

def dequantize(quantized_tensor, scale, zero_point):

    return (quantized_tensor.float() - zero_point) * scale

def calculate_model_memory(model):

    return sum(param.element_size() * param.numel() for param in model.parameters()) / (1024 ** 2)

# Create a deep copy of the model for quantization
quantized_model = copy.deepcopy(model)

# Compute and print memory usage before quantization
original_memory = calculate_model_memory(model)
print(f"Memory usage before quantization: {original_memory:.2f} MB")

# Selective quantization for specific parameters
quantized_params = {}
for param_name, param in quantized_model.named_parameters():
    if "mlp" in param_name:  # Quantize only Feed-Forward Network (FFN) layers
        param_min, param_max = param.min(), param.max()
        scale, zero_point = compute_quantization_params(param_min, param_max)
        quantized_data = quantize(param.data, scale, zero_point)
        quantized_params[param_name] = (quantized_data, scale, zero_point)
    else:
        # Retain other parameters in original format
        quantized_params[param_name] = (param.data, None, None)

# Calculate and print memory usage after selective quantization
quantized_memory = sum(
    quant_data[0].element_size() * quant_data[0].numel() for quant_data in quantized_params.values()
) / (1024 ** 2)
print(f"Memory usage after selective quantization: {quantized_memory:.2f} MB")

import torch
import time

def evaluate_latency(model, tokenizer, input_text, device="cpu"):

    tokenized_inputs = tokenizer(input_text, return_tensors="pt").to(device)
    model = model.to(device)
    start_time = time.time()

    with torch.no_grad():
        model.generate(**tokenized_inputs, max_new_tokens=50)

    elapsed_time = time.time() - start_time
    return elapsed_time

# Determine the appropriate device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Move models to the selected device
model = model.to(device)
quantized_model = quantized_model.to(device)

# Measure latency for the original model
sample_text = "Taj Mahal is located in"
latency_original = evaluate_latency(model, tokenizer, sample_text, device)
print(f"Inference latency before quantization: {latency_original:.4f} seconds")

# Replace parameters in quantized_model with dequantized versions
for param_name, param in quantized_model.named_parameters():
    if param_name in quantized_params:
        quantized_data, scale, zero_point = quantized_params[param_name]
        if scale is not None and zero_point is not None:
            # Convert quantized data back to floating-point representation
            param.data = dequantize(quantized_data, scale, zero_point).to(param.data.dtype)
        else:
            # Retain original parameter if no quantization was applied
            param.data = quantized_data.to(param.data.dtype)

# Measure latency for the quantized model
latency_quantized = evaluate_latency(quantized_model, tokenizer, sample_text, device)
print(f"Inference latency after quantization: {latency_quantized:.4f} seconds")

! pip install datasets

from datasets import load_dataset
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import math

# Determine device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load the WikiText-2 dataset (test split)
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
filtered_data = [line for line in dataset["text"] if line.strip()]
sample_data = filtered_data[:3000]

def compute_perplexity(model, tokenizer, dataset):

    model.eval()
    total_loss = 0
    total_tokens = 0

    with torch.no_grad():
        for text in dataset:
            try:
                # Tokenize and process input text
                tokenized_inputs = tokenizer(
                    text,
                    return_tensors="pt",
                    truncation=True,
                    padding=True,
                    max_length=tokenizer.model_max_length
                ).to(device)

                # Skip empty or invalid tokenized inputs
                if tokenized_inputs["input_ids"].size(1) == 0:
                    print(f"Skipping invalid or empty input: {text}")
                    continue

                # Clone inputs for label creation
                labels = tokenized_inputs["input_ids"].clone()

                # Perform forward pass
                outputs = model(**tokenized_inputs, labels=labels)

                # Extract loss
                loss = outputs.loss

                # Check for valid loss values
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"Invalid loss encountered for input: {text}")
                    continue

                # Accumulate total loss and token count
                total_loss += loss.item() * labels.size(1)
                total_tokens += labels.size(1)

            except Exception as error:
                print(f"Error processing input: {text}. Error: {error}")
                continue

    # Ensure valid tokens were processed
    if total_tokens == 0:
        raise ValueError("No valid sentences found for perplexity computation.")

    # Calculate perplexity
    perplexity = math.exp(total_loss / total_tokens)
    return perplexity


# Ensure tokenizer has a pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Move quantized model to the appropriate device
quantized_model = quantized_model.to(device)

# Calculate perplexity for the quantized model
print("Evaluating perplexity for the selective quantized model on WikiText-2...")
quantized_model_perplexity = compute_perplexity(quantized_model, tokenizer, sample_data)
print(f"Perplexity after quantization: {quantized_model_perplexity:.4f}")

import matplotlib.pyplot as plt

# Define metrics and their corresponding values
quantization_methods = ['Before Quantization', 'Whole-Model Quantization', 'Selective Quantization']
memory_usage = [5018.52, 1254.63, 2713.82]
inference_latency = [2.4626, 1.4988, 1.5046]
model_perplexity = [25.0811, 26.8221, 25.8709]

# Plot Memory Usage
plt.figure(figsize=(8, 6))
plt.bar(quantization_methods, memory_usage, color=['blue', 'orange', 'green'])
plt.xlabel("Quantization Method")
plt.ylabel("Memory Usage (MB)")
plt.title("Memory Usage Across Quantization Methods")
plt.show()

# Plot Inference Latency
plt.figure(figsize=(8, 6))
plt.bar(quantization_methods, inference_latency, color=['blue', 'orange', 'green'])
plt.xlabel("Quantization Method")
plt.ylabel("Latency (seconds)")
plt.title("Inference Latency Across Quantization Methods")
plt.show()

# Plot Model Perplexity
plt.figure(figsize=(8, 6))
plt.bar(quantization_methods, model_perplexity, color=['blue', 'orange', 'green'])
plt.xlabel("Quantization Method")
plt.ylabel("Perplexity")
plt.title("Model Perplexity Across Quantization Methods")
plt.show()


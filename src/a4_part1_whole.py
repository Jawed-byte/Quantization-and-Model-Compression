# -*- coding: utf-8 -*-
"""A4_Part1_whole.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aq9ab-nnYM_h3xEtKLlDQPAhN-CT4kDn
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import time
import copy

model_name = "EleutherAI/gpt-neo-1.3B"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

def compute_quantization_params(min_val, max_val, dtype=torch.int8):
    """Calculate scale and zero-point for quantization."""
    qmin = torch.iinfo(dtype).min
    qmax = torch.iinfo(dtype).max
    scale = (max_val - min_val) / (qmax - qmin)
    zero_point = qmin - (min_val / scale)
    return scale, zero_point

def quantize(tensor, scale, zero_point):
    """Quantize tensor to int8 using scale and zero_point."""
    quantized_tensor = ((tensor / scale) + zero_point).round().to(torch.int8)
    return quantized_tensor

def dequantize(quantized_tensor, scale, zero_point):
    """Dequantize tensor back to float32."""
    return (quantized_tensor.float() - zero_point) * scale

def calculate_model_memory(model):
    """Calculate the memory size of the model in MB."""
    return sum([param.element_size() * param.numel() for param in model.parameters()]) / (1024 ** 2)

quantized_model = copy.deepcopy(model)

# Memory usage before quantization
memory_before = calculate_model_memory(model)
print(f"Memory before quantization: {memory_before:.2f} MB")

# Whole-Model Quantization (applying quantization to all layers in the copied model)
quantized_model_params = {}
for name, param in quantized_model.named_parameters():
    min_val, max_val = param.min(), param.max()
    scale, zero_point = compute_quantization_params(min_val, max_val)
    quantized_param = quantize(param.data, scale, zero_point)
    quantized_model_params[name] = (quantized_param, scale, zero_point)

# Memory usage after quantization
memory_after = sum([q_param[0].element_size() * q_param[0].numel() for q_param in quantized_model_params.values()]) / (1024 ** 2)
print(f"Memory after quantization: {memory_after:.2f} MB")

import torch
import time

def evaluate_latency(model, tokenizer, text, device="cpu"):
    """Measure inference latency of the model."""
    inputs = tokenizer(text, return_tensors="pt").to(device)
    model = model.to(device)
    start_time = time.time()

    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=50)  # Generate tokens

    end_time = time.time()
    return end_time - start_time


device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)
quantized_model = quantized_model.to(device)

# Measure latency before quantization
text = "Taj Mahal is located in "
latency_before = evaluate_latency(model, tokenizer, text, device)
print(f"Inference latency before quantization: {latency_before:.4f} seconds")

for name, param in quantized_model.named_parameters():
    if name in quantized_model_params:
        quantized_tensor, scale, zero_point = quantized_model_params[name]
        # Dequantize to floating point for PyTorch compatibility
        param.data = dequantize(quantized_tensor, scale, zero_point).to(param.data.dtype)

# Measure latency after whole-model quantization
latency_after = evaluate_latency(quantized_model, tokenizer, text, device)
print(f"Inference latency after quantization: {latency_after:.4f} seconds")

!pip install datasets

from datasets import load_dataset
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import math

device = "cuda" if torch.cuda.is_available() else "cpu"

wikitext = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
data = [entry for entry in wikitext["text"] if entry.strip()]
data = data[:3000]


def compute_perplexity(model, tokenizer, data):
    """Calculate perplexity of the model on given data."""
    model.eval()
    total_loss = 0
    total_words = 0

    with torch.no_grad():
        for text in data:
            try:

                inputs = tokenizer(
                    text,
                    return_tensors="pt",
                    truncation=True,
                    padding=True,
                    max_length=tokenizer.model_max_length
                ).to(device)


                if inputs["input_ids"].size(1) == 0:
                    print(f"Skipping empty or invalid input: {text}")
                    continue

                labels = inputs["input_ids"].clone()


                outputs = model(**inputs, labels=labels)


                loss = outputs.loss


                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"Invalid loss detected for input: {text}")
                    continue


                total_loss += loss.item() * labels.size(1)
                total_words += labels.size(1)

            except Exception as e:
                print(f"Error processing input: {text}. Error: {e}")
                continue


    if total_words == 0:
        raise ValueError("No valid sentences found for perplexity calculation.")


    perplexity = math.exp(total_loss / total_words)
    return perplexity

model = model.to(device)
quantized_model = quantized_model.to(device)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Calculate perplexity for the original model
print("Calculating perplexity for the original model on WikiText-2...")
perplexity_before = compute_perplexity(model, tokenizer, data)
print(f"Perplexity before quantization: {perplexity_before:.4f}")

# Calculate perplexity for the quantized model
print("Calculating perplexity for the quantized model on WikiText-2...")
perplexity_after = compute_perplexity(quantized_model, tokenizer, data)
print(f"Perplexity after quantization: {perplexity_after:.4f}")


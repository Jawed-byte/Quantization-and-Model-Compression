# -*- coding: utf-8 -*-
"""A4_Part2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yVhOrt4kjIsv4oaYBRlRKhkmFzIs5iKX
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install bitsandbytes datasets

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/ANLP_A4/bnb/

import torch
import bitsandbytes as bnb
from transformers import GPT2LMHeadModel, GPT2Tokenizer, BitsAndBytesConfig, AutoModelForCausalLM
from datasets import load_dataset
from torch.nn import CrossEntropyLoss
import math
from tqdm import tqdm
import json
import os
import time
import numpy as np
import matplotlib.pyplot as plt

def calculate_model_size(model):
    """Calculate model size in MB"""
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    return param_size / (1024 * 1024)  # Convert to MB

def calculate_perplexity(model, tokenizer, eval_data, device, max_length=1024):
    """Calculate perplexity on evaluation data"""
    model.eval()
    total_loss = 0.0
    total_length = 0
    loss_fn = CrossEntropyLoss(reduction='sum')

    with torch.no_grad():
        for text in tqdm(eval_data, desc="Calculating perplexity"):
            encodings = tokenizer(text,
                                return_tensors='pt',
                                max_length=max_length,
                                truncation=True)

            input_ids = encodings.input_ids.to(device)
            attention_mask = encodings.attention_mask.to(device)
            labels = input_ids.clone()

            outputs = model(input_ids=input_ids,
                          attention_mask=attention_mask,
                          labels=labels)

            shift_logits = outputs.logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()

            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)),
                         shift_labels.view(-1))

            total_loss += loss.item()
            total_length += attention_mask.sum().item() - 1

    try:
        perplexity = math.exp(total_loss / total_length)
    except OverflowError:
        perplexity = float('inf')

    return perplexity

def measure_inference_metrics(model, tokenizer, input_text, device, num_runs=10):
    """Measure inference latency and memory usage"""
    model.eval()
    cuda_available = torch.cuda.is_available()

    # Prepare input
    inputs = tokenizer(input_text, return_tensors="pt").to(device)

    # Measure memory before inference
    if cuda_available:
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.empty_cache()
        initial_memory = torch.cuda.memory_allocated() / (1024 * 1024)  # MB

    # Measure inference time
    latencies = []
    with torch.no_grad():
        for _ in range(num_runs):
            start_time = time.time()
            _ = model(**inputs)
            end_time = time.time()
            latencies.append((end_time - start_time) * 1000)  # Convert to ms

    # Get memory stats
    if cuda_available:
        current_memory = torch.cuda.memory_allocated() / (1024 * 1024)  # MB
        peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024)  # MB
        memory_footprint = current_memory - initial_memory
    else:
        memory_footprint = 0
        peak_memory = 0

    metrics = {
        'inference_time': {
            'mean': np.mean(latencies),
            'std': np.std(latencies)
        },
        'memory_footprint': memory_footprint,
        'peak_memory': peak_memory
    }

    return metrics

def plot_comparison_graphs(results):
    """Create comparison plots for all models"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

    # Prepare data
    display_names = ['Original', '8-bit', '4-bit Linear', '4-bit NF4']
    model_keys = ['original', 'quantized_8bit', 'quantized_4bit_linear', 'quantized_4bit_nf4']

    sizes = [results['model_sizes'][k] for k in ['original_size_mb', '8bit_size_mb', '4bit_linear_size_mb', '4bit_nf4_size_mb']]
    perplexities = [results['perplexity'][k] for k in ['original', '8bit', '4bit_linear', '4bit_nf4']]
    inference_times = [results['inference_metrics'][k]['inference_time']['mean'] for k in model_keys]
    memory_footprints = [results['inference_metrics'][k]['memory_footprint'] for k in model_keys]

    # Color scheme
    bar_color = '#2ecc71'

    # 1. Model Sizes
    bars1 = ax1.bar(display_names, sizes, color=bar_color)
    ax1.set_title('Model Sizes')
    ax1.set_ylabel('Size (MB)')
    ax1.tick_params(axis='x', rotation=45)
    ax1.grid(True, linestyle='--', alpha=0.7)

    # 2. Perplexity Comparison
    bars2 = ax2.bar(display_names, perplexities, color=bar_color)
    ax2.set_title('Perplexity Scores')
    ax2.set_ylabel('Perplexity')
    ax2.tick_params(axis='x', rotation=45)
    ax2.grid(True, linestyle='--', alpha=0.7)

    # 3. Inference Time
    bars3 = ax3.bar(display_names, inference_times, color=bar_color)
    ax3.set_title('Average Inference Time')
    ax3.set_ylabel('Time (ms)')
    ax3.tick_params(axis='x', rotation=45)
    ax3.grid(True, linestyle='--', alpha=0.7)

    # 4. Memory Footprint
    bars4 = ax4.bar(display_names, memory_footprints, color=bar_color)
    ax4.set_title('Memory Footprint')
    ax4.set_ylabel('Memory (MB)')
    ax4.tick_params(axis='x', rotation=45)
    ax4.grid(True, linestyle='--', alpha=0.7)

    # Add value labels on top of bars
    def add_value_labels(ax, bars):
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.1f}',
                   ha='center', va='bottom')

    add_value_labels(ax1, bars1)
    add_value_labels(ax2, bars2)
    add_value_labels(ax3, bars3)
    add_value_labels(ax4, bars4)

    plt.suptitle('Model Comparison Across Different Metrics', fontsize=16, y=0.95)
    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

def save_models(models_dict, save_dir="quantized_models"):
    """Save all models to specified directory"""
    os.makedirs(save_dir, exist_ok=True)

    for model_name, model in models_dict.items():
        print(f"\nSaving {model_name}...")
        model_path = os.path.join(save_dir, model_name)
        os.makedirs(model_path, exist_ok=True)
        model.save_pretrained(model_path)
        print(f"Model saved to {model_path}")

def load_and_evaluate_models():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    print("\nLoading test dataset...")
    dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")

    eval_texts = []
    for item in dataset:
        if item['text'].strip():
            eval_texts.append(item['text'])
            if len(eval_texts) >= 3000:
                break
    print(f"Loaded {len(eval_texts)} samples for evaluation")

    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    tokenizer.pad_token = tokenizer.eos_token

    # Load all model variants
    print("\nLoading original model...")
    original_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)

    print("\nLoading 8-bit quantized model...")
    quantization_config_8bit = BitsAndBytesConfig(
        load_in_8bit=True,
        llm_int8_threshold=200.0
    )
    model_8bit = GPT2LMHeadModel.from_pretrained(
        'gpt2',
        quantization_config=quantization_config_8bit,
        device_map='auto'
    )

    print("\nLoading 4-bit linear quantized model...")
    quantization_config_4bit_linear = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="fp4",
        bnb_4bit_compute_dtype=torch.float16
    )
    model_4bit_linear = GPT2LMHeadModel.from_pretrained(
        'gpt2',
        quantization_config=quantization_config_4bit_linear,
        device_map='auto'
    )

    print("\nLoading 4-bit NF4 quantized model...")
    quantization_config_4bit_nf4 = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )
    model_4bit_nf4 = GPT2LMHeadModel.from_pretrained(
        'gpt2',
        quantization_config=quantization_config_4bit_nf4,
        device_map='auto'
    )

    # Save all models
    models_dict = {
        'original': original_model,
        'quantized_8bit': model_8bit,
        'quantized_4bit_linear': model_4bit_linear,
        'quantized_4bit_nf4': model_4bit_nf4
    }
    save_models(models_dict)

    # Calculate sizes
    print("\nCalculating model sizes...")
    sizes = {
        'original': calculate_model_size(original_model),
        '8bit': calculate_model_size(model_8bit),
        '4bit_linear': calculate_model_size(model_4bit_linear),
        '4bit_nf4': calculate_model_size(model_4bit_nf4)
    }

    # Measure inference metrics
    print("\nMeasuring inference metrics...")
    sample_text = "This is a sample text to measure inference performance. " * 10  # Longer text for better measurement
    inference_metrics = {}

    for model_name, model in models_dict.items():
        print(f"\nMeasuring metrics for {model_name}...")
        inference_metrics[model_name] = measure_inference_metrics(
            model, tokenizer, sample_text, device
        )

    # Evaluate all models
    print("\nEvaluating models...")
    perplexities = {
        'original': calculate_perplexity(original_model, tokenizer, eval_texts, device),
        '8bit': calculate_perplexity(model_8bit, tokenizer, eval_texts, device),
        '4bit_linear': calculate_perplexity(model_4bit_linear, tokenizer, eval_texts, device),
        '4bit_nf4': calculate_perplexity(model_4bit_nf4, tokenizer, eval_texts, device)
    }

    # Compile results
    results = {
        'num_samples': len(eval_texts),
        'model_sizes': {
            'original_size_mb': float(sizes['original']),
            '8bit_size_mb': float(sizes['8bit']),
            '4bit_linear_size_mb': float(sizes['4bit_linear']),
            '4bit_nf4_size_mb': float(sizes['4bit_nf4'])
        },
        'perplexity': {
            #'originect. It is likely designed to provide utility functions that are used across different parts of the code. Specifically, the codal'
            'original': float(perplexities['original']),
            '8bit': float(perplexities['8bit']),
            '4bit_linear': float(perplexities['4bit_linear']),
            '4bit_nf4': float(perplexities['4bit_nf4'])
        },
        'inference_metrics': inference_metrics
    }

    # Create comparison plots
    plot_comparison_graphs(results)

    # Print results
    print("\nEvaluation Results:")
    print(f"Number of samples: {results['num_samples']}")
    print("\nModel Sizes:")
    print(f"Original model: {results['model_sizes']['original_size_mb']:.2f} MB")
    print(f"8-bit model: {results['model_sizes']['8bit_size_mb']:.2f} MB")
    print(f"4-bit linear model: {results['model_sizes']['4bit_linear_size_mb']:.2f} MB")
    print(f"4-bit NF4 model: {results['model_sizes']['4bit_nf4_size_mb']:.2f} MB")

    print("\nPerplexity Scores:")
    print(f"Original model: {results['perplexity']['original']:.2f}")
    print(f"8-bit model: {results['perplexity']['8bit']:.2f}")
    print(f"4-bit linear model: {results['perplexity']['4bit_linear']:.2f}")
    print(f"4-bit NF4 model: {results['perplexity']['4bit_nf4']:.2f}")

    print("\nInference Metrics:")
    for model_name in models_dict.keys():
        print(f"\n{model_name}:")
        print(f"Average Inference Time: {inference_metrics[model_name]['inference_time']['mean']:.2f} ms")
        print(f"Memory Footprint: {inference_metrics[model_name]['memory_footprint']:.2f} MB")
        print(f"Peak Memory: {inference_metrics[model_name]['peak_memory']:.2f} MB")

    # Save results
    with open('quantization_results.json', 'w') as f:
        json.dump(results, f, indent=4)
    print("\nResults have been saved to 'quantization_results.json'")

    return results

results = load_and_evaluate_models()

